<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0023)https://jonbarron.info/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="“width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    subtitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 18px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="https://jonbarron.info/seal_icon.png">
  <title>Qingnan Fan (樊庆楠)</title>
  
  <link href="./QingnanFan_files/css" rel="stylesheet" type="text/css">
  <script charset="utf-8" src="chrome-extension://jgphnjokjhjlcnnajmfjlacjnjkhleah/js/btype.js"></script></head>

  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="100%" valign="middle">
        <p align="center">
          <name>Qingnan Fan (樊庆楠)</name>
        </p>
        <p>
          I am a Lead Researcher and team manager in the Imaging Algorithm Center of VIVO. 
          Our group is the core algorithm team responsible for advancing the photographic quality in the flagship smartphones with the cutting-edge technologies (3D, AIGC, etc).
        </p>
        <p>
          I was a Senior Researcher in the Visual Computing Center of Tencent AI Lab between 2021 to 2023.
        </p>
        <p>
          I was a Postdoctoral Researcher in Stanford University supervised by <a
              href="https://geometry.stanford.edu/member/guibas/"> Prof. Leonidas Guibas</a> between 2019 to 2021.
        </p>
        <p> I obtained my PhD degree in the  <a
                href="http://www.cs.sdu.edu.cn/">Computer Science and Technology School</a> of <a
                href="http://www.sdu.edu.cn/">Shandong University</a> at 2019. I was supervised by <a
                href="http://www.cs.sdu.edu.cn/~baoquan/"> Prof. Baoquan Chen</a>.
        </p>

        <p> <strong>If you are interested in the internship in our group for either publishing academic papers or working on engineering projects, feel free to drop me an email.</strong> </p>

        <!-- <p>
            I was a research intern in the Visual Computing Group of MSRA supervised by <a href="http://www.davidwipf.com/">David Wipf</a> from Sept. 2016 to Feb. 2018. I also collaborated with <a href="https://www.microsoft.com/en-us/research/people/xtong/">Xin Tong</a>, <a href="https://www.microsoft.com/en-us/research/people/ganghua/">Gang Hua</a> and <a href="http://jlyang.org/">Jiaolong Yang</a> while in MSR.
        </p>
        <p>
            I was also a research intern in the Advanced Innovation Center for Future Visual Entertainment led by <a
                href="http://www.cs.sdu.edu.cn/~baoquan/"> Prof. Baoquan Chen</a>, in Beijing Film Academy between Mar. 2018 and Aug. 2019.
        </p>
        <p>
          I visited Tel Aviv University, Hebrew University of Jerusalem several times between 2014 to 2015 to work with 
              <a href="http://www.math.tau.ac.il/~dcor/">Prof. Daniel Cohen-Or</a> and 
              <a href="http://www.cs.huji.ac.il/~danix/">Prof. Dani Lischinski</a>.
        </p> -->
        <p align="center">
          <a href="mailto:fqnchina@gmail.com">Email</a> &nbsp/&nbsp
          <a href="./QingnanFan_files/qingnan_cv.pdf">CV</a> &nbsp/&nbsp
          <a href="./QingnanFan_files/Qingnan-bio.txt">Biography</a> &nbsp/&nbsp
          <a href="https://scholar.google.co.uk/citations?user=2cY2zwUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
          <a href="https://www.linkedin.com/in/qingnan-fan-02140aa7/">LinkedIn</a> &nbsp/&nbsp
          <a href="https://twitter.com/FanQingnan/">Twitter</a> &nbsp/&nbsp
          <a href="https://github.com/fqnchina/">Github</a>
        </p>
        </td>
        <td width="33%">
        <img src="./QingnanFan_files/qingnan_circle_v3.jpg" width="250" alt="" style="border-style: none" align="middle">
        </td>
      </tr>
      </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td width="100%" valign="middle">
            <strong><heading>Tech transfer</heading></strong>
            <p>
              <p>VIVO X200 series: <a href="https://www.vivo.com.cn/brand/news/detail?id=1273&type=0">Telephoto Image Enhancement​</a></p>
            </p>
          </td>
        </tr>
        </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td width="100%" valign="middle">
            <strong><heading>BlueTalk</heading></strong>
            <p>
              I am the host of BlueTalk, which invites the expert speakers in the fields of computer vision, graphics and computational photography. The mission of BlueTalk is to promote the communication between academic and industrial communities, and explore the possibilities of landing the most innovative research ideas into any AI-powered industrial applications.
              If you are insterested in joining this family to give a talk, feel free to let me know.
              <p>2024-12-09: <a href="https://scholar.google.com.hk/citations?user=mmtNSisAAAAJ&hl=zh-CN">Du Chen</a>, PolyU, Real-world image super-resolution: solutions and challenges</p>
              <p>2024-10-30: <a href="https://www.mengweiren.com/">Mengwei Ren</a>, Adobe, Relightful harmonization: lighting-aware portrait background replacement</p>
              <p>2024-08-26: <a href="https://scholar.google.com/citations?user=kQUJjQQAAAAJ&hl=en">Jianqi Ma</a>, PolyU, AI vision research based on text</p>
              <p>2023-09-08: <a href="https://xinntao.github.io/">Xintao Wang</a>, Tencent, Visual generation and editing via diffusion models</p>
              <p>2023-08-15: <a href="http://www.cs.toronto.edu/~linghuan/">Huan Ling</a>, Nvidia, Generative models and vision perception using diffusion models</p>
              <p>2023-07-25: <a href="https://jianminbao.github.io/">Jianmin Bao</a>, MSRA, 2D & 3D visual synthesis and manipulation via diffusion models</p>
            </p>
          </td>
        </tr>
        </tbody></table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="100%" valign="middle">
          <strong><heading>Selected publications</heading></strong>
          <p>
            My research focus lies in computer graphics, 3D vision, image processing, and human-computer interaction. My recent effort has been spent on pushing the limit of 3D vision and reinforcement learning technologies to implement an intelligent embodied agent in both forms of physical robots and digital humans. <strong>For the complete publication list, please refer to my <a href="https://scholar.google.co.uk/citations?user=2cY2zwUAAAAJ&hl=en">google scholar page</a>.</strong>
          </p>
        </td>
      </tr>
      </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id="portrait_image" style="opacity: 0;">
            <img src="./QingnanFan_files/cvpr_2025.gif" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <img src="./QingnanFan_files/cvpr_2025.gif" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <script type="text/javascript">
            function portrait_start() {
            document.getElementById('portrait_image').style.opacity = "1";
            }
            function portrait_stop() {
            document.getElementById('portrait_image').style.opacity = "0";
            }
            portrait_stop()
            </script>
          </td>
          <td valign="top" width="75%">
            <a href="./QingnanFan_files/cvpr_2025.pdf">
              <papertitle>SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos</papertitle>
            </a>
        <br>     
        Yuzheng Liu*,
        <a href="https://scholar.google.com/citations?user=vtZMhssAAAAJ&hl=en/">Siyan Dong*</a>, 
        <a href="https://ffrivera0.github.io/">Shuzhe Wang</a>, 
        <a href="https://yd-yin.github.io/">Yingda Yin</a>, 
        <a href="https://yanchaoyang.github.io/">Yanchao Yang</a>, 
        <strong>Qingnan Fan</strong>, 
        <a href="https://cfcs.pku.edu.cn/baoquan/">Baoquan Chen</a>.
        <br>
            <em>CVPR</em>, 2025 <strong style="color:red">(Highlight)</strong></strong><br>
            <a href="https://arxiv.org/abs/2412.09401" target="_blank">arXiv</a>
            /
            <a href="https://www.youtube.com/watch?v=V1SHYkCTqHc" target="_blank">video</a>
            /
            <a href="https://github.com/PKU-VCL-3DV/SLAM3R/" target="_blank">codes</a>
            <p></p>
            <p> SLAM3R is a real-time dense scene reconstruction system that regresses 3D points from video frames using feed-forward neural networks, without explicitly estimating camera parameters.</p>
          </td>
        </tr>


        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id="portrait_image" style="opacity: 0;">
            <img src="./QingnanFan_files/siga_2023_activity_small.gif" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <img src="./QingnanFan_files/siga_2023_activity_small.gif" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <script type="text/javascript">
            function portrait_start() {
            document.getElementById('portrait_image').style.opacity = "1";
            }
            function portrait_stop() {
            document.getElementById('portrait_image').style.opacity = "0";
            }
            portrait_stop()
            </script>
          </td>
          <td valign="top" width="75%">
            <!-- <a href="./QingnanFan_files/siggraphasia_2023.pdf"></a> -->
            <papertitle>Scene-aware Activity Program Generation with Language Guidance</papertitle>
        <br>     
        Zejia Su,
        <strong>Qingnan Fan</strong>, 
        <a href="https://xuelin-chen.github.io/">Xuelin Chen</a>, 
        <a href="https://people.scs.carleton.ca/~olivervankaick/">Oliver van Kaick</a>, 
        <a href="https://vcc.tech/~huihuang">Hui Huang</a>, 
        <a href="https://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>.
        <br>
            <em>SIGGRAPH Asia</em>, 2023 & TOG, 2023</strong><br>
            <a href="https://toddbear.github.io/LangGuidedProg/" target="_blank">project page</a>
            /
            <a href="./QingnanFan_files/tog_2023_supp.pdf" target="_blank">supp file</a>
            /
            <a href="./QingnanFan_files/tog_2023.bib">bibtex</a>
            <p></p>
            <p> We address the problem of scene-aware activity program generation, which requires decomposing a given activity task into instructions that can be sequentially performed within a target scene to complete the activity.</p>
          </td>
        </tr>


        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id="portrait_image" style="opacity: 0;">
            <img src="./QingnanFan_files/siga_2023_ease.gif" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <img src="./QingnanFan_files/siga_2023_ease.gif" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <script type="text/javascript">
            function portrait_start() {
            document.getElementById('portrait_image').style.opacity = "1";
            }
            function portrait_stop() {
            document.getElementById('portrait_image').style.opacity = "0";
            }
            portrait_stop()
            </script>
          </td>
          <td valign="top" width="75%">
            <a href="./QingnanFan_files/siga_2023.pdf">
              <papertitle>C·ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters</papertitle>
            </a>
        <br>     
        <a href="https://frank-zy-dou.github.io/">Zhiyang Dou</a>,
        <a href="https://xuelin-chen.github.io/">Xuelin Chen</a>, 
        <strong>Qingnan Fan</strong>, 
        <a href="https://homepages.inf.ed.ac.uk/tkomura/">Taku Komura</a>, 
        <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html">Wenping Wang</a>.
        <br>
            <em>SIGGRAPH Asia</em>, 2023</strong><br>
            <a href="http://arxiv.org/abs/2309.11351" target="_blank">arXiv</a>
            /
            <a href="https://frank-zy-dou.github.io/projects/CASE/index.html" target="_blank">project page</a>
            /
            <a href="https://youtu.be/Cgq6JbQ1VW4" target="_blank">video</a>
            /
            <a href="./QingnanFan_files/siga_2023.bib">bibtex</a>
            <p></p>
            <p> We present C·ASE, an efficient and effective framework that learns conditional Adversarial Skill Embeddings for Elite physics-based characters.</p>
          </td>
        </tr>



    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/cvpr22_adela.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/cvpr22_adela.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/cvpr_2022_adela.pdf">
          <papertitle>ADeLA: Automatic Dense Labeling with Attention for Viewpoint Shift in Semantic Segmentation</papertitle>
    </a>
    <br>     
    <a href="https://yanchaoyang.github.io/">Yanchao Yang*</a>, 
    Hanxiang Ren*, 
    <a href="https://hughw19.github.io/">He Wang</a>, 
    <a href="https://cs.stanford.edu/people/bshen88/">Bokui Shen</a>, 
    <strong>Qingnan Fan</strong>, 
    <a href="https://www.youyizheng.net/">Youyi Zheng</a>, 
    <a href="https://ckllab.stanford.edu/">C. Karen Liu</a>,
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas Guibas</a>.
    <br>
        <em>CVPR</em>, 2022 <strong style="color:red">(Oral)</strong><br>
        <a href="https://arxiv.org/abs/2107.14285" target="_blank">arXiv</a>
        /
        <a href="./QingnanFan_files/cvpr_2022_adela.bib">bibtex</a>
        <p></p>
        <p> We describe a method to deal with performance drop in semantic segmentation caused by viewpoint changes within multi-camera systems, where temporally paired images are readily available, but the annotations may only be abundant for a few typical views.</p>
      </td>
    </tr>

    
    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/iccv_2021_captra3.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/iccv_2021_captra3.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/iccv_2021_captra.pdf">
            <papertitle>CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated Objects from Point Clouds</papertitle>
    </a>
    <br>          
    Yijia Weng*, 
    <a href="https://hughw19.github.io/">He Wang*</a>, 
    Qiang Zhou,
    <a href="https://yzqin.github.io/">Yuzhe Qin</a>, 
    <a href="https://geometry.stanford.edu/person.php?id=duanyq19">Yueqi Duan</a>, 
    <strong>Qingnan Fan</strong>, 
    <a href="https://cfcs.pku.edu.cn/baoquan/">Baoquan Chen</a>,
    <a href="http://ai.ucsd.edu/~haosu/">Hao Su</a>,
    <a href="https://geometry.stanford.edu/member/guibas/index.html">Leonidas Guibas</a>.
    <br>
        <em>ICCV</em>, 2021 <strong style="color:red">(Oral)</strong><br>
        <a href="https://arxiv.org/abs/2104.03437">arXiv</a>
        /
        <a href="https://yijiaweng.github.io/CAPTRA/" target="_blank">project page</a>
        /
        <a href="https://github.com/HalfSummer11/CAPTRA">codes</a>
        /
        <a href="https://www.youtube.com/watch?v=JFPcOHCH2O0">video</a>
        /
        <a href="./QingnanFan_files/iccv_2021_captra.bib">bibtex</a>
        <p></p>
        <p> For the first time, we propose a unified framework that can handle 9-DoF pose tracking for novel rigid object instances as well as per-part pose tracking for 3D articulated objects.</p>
      </td>
    </tr>




    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/cvpr21_localization_thumbnail.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/cvpr21_localization_thumbnail.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/cvpr_2021_localization.pdf">
            <papertitle>Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments</papertitle>
    </a>
    <br>          
    <a href="https://scholar.google.com/citations?user=vtZMhssAAAAJ&hl=en/">Siyan Dong*</a>, 
    <strong>Qingnan Fan*</strong>, 
    <a href="https://hughw19.github.io/">He Wang</a>, 
    Ji Shi,
    <a href="https://ericyi.github.io/">Li Yi</a>, 
    <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>,
    <a href="https://cfcs.pku.edu.cn/baoquan/">Baoquan Chen</a>,
    <a href="https://geometry.stanford.edu/member/guibas/index.html">Leonidas Guibas</a>.
    <br>
        <em>CVPR</em>, 2021 <strong style="color:red">(Oral)</strong><br>
        <a href="https://arxiv.org/abs/2012.04746">arXiv</a>
        /
        <a href="https://github.com/siyandong/NeuralRouting">codes</a>
        /
        <a href="https://www.youtube.com/watch?v=_1eInWKbuVA" target="_blank">video</a>
        /
        <a href="./QingnanFan_files/cvpr_2021_localization.bib">bibtex</a>
        <p></p>
        <p> A novel outlier-aware neural tree to tackle the camera localization challenges in dynamic indoor environments. It achieves the best performance in the RIO-10 benchmark.</p>
      </td>
    </tr>


    <tr onmouseout="siga18_stop()" onmouseover="siga18_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="siga18_thumbnail" style="opacity: 0;">
          <img src="./QingnanFan_files/siga18_thumbnail_before.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/siga18_thumbnail_after.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function siga18_start() {
        document.getElementById('siga18_thumbnail').style.opacity = "1";
        }
        function siga18_stop() {
        document.getElementById('siga18_thumbnail').style.opacity = "0";
        }
        siga18_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/siggraph_asia_2018.pdf">
            <papertitle>Image Smoothing via Unsupervised Learning</papertitle>
    </a>
    <!-- <papertitle>Image Smoothing via Unsupervised Learning</papertitle> -->
    <br>
        <strong>Qingnan Fan</strong>, 
        <a href="http://jlyang.org/">Jiaolong Yang</a>, 
        <a href="http://www.davidwipf.com/">David Wipf</a>,        
        <a href="http://www.cs.sdu.edu.cn/~baoquan/">Baoquan Chen</a>,
        <a href="https://www.microsoft.com/en-us/research/people/xtong/">Xin Tong</a>.
    <br>
        <em>SIGGRAPH Asia</em>, 2018 & <em>TOG</em>, 2018 <br>
        <a href="https://arxiv.org/abs/1811.02804">arXiv</a>
        / 
        <a href="https://github.com/fqnchina/ImageSmoothing">codes</a>
        / 
        <a href="https://www.dropbox.com/s/p3ql7etstto1g4e/siggraph_asia_2018_supp.pdf?dl=0">supp file</a>
        /
        <a href="./QingnanFan_files/siga_2018.bib">bibtex</a>
        <p></p>
        <p> Treat deep learning as an optimization tool to minimize the proposed image smoothing objective function in an unsupervised manner. Multiple different smoothing effects can be easily learned by adaptively changing the proposed objective function.</p>
      </td>
    </tr>


    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
          <img src="./QingnanFan_files/cvpr18_thumbnail.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/cvpr18_thumbnail.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
      <a href="./QingnanFan_files/cvpr_2018.pdf">
            <papertitle>Revisiting Deep Intrinsic Image Decompositions</papertitle>
      </a>
      <br>
        <strong>Qingnan Fan</strong>, 
        <a href="http://jlyang.org/">Jiaolong Yang</a>, 
        <a href="https://www.microsoft.com/en-us/research/people/ganghua/">Gang Hua</a>, 
        <a href="http://www.cs.sdu.edu.cn/~baoquan/">Baoquan Chen</a>,
        <a href="http://www.davidwipf.com/">David Wipf</a>.

        <br>
        <em>CVPR</em>, 2018 <strong style="color:red">(Oral)</strong><br>
        <a href="https://arxiv.org/abs/1701.02965">arXiv</a>
        / 
        <a href="https://github.com/fqnchina/IntrinsicImage">codes</a>
        /
        <a href="./QingnanFan_files/cvpr_2018_v4_JL.pptx">slides</a>
        /  
        <a href="./QingnanFan_files/cvpr_2018_supp.pdf">supp file</a>
        /
        <a href="./QingnanFan_files/cvpr_2018_poster.pdf">poster</a>
        / 
        <a href="https://www.youtube.com/watch?v=LBJ20kxr1a0&t=3026s">presentation (start from 36:44)</a>
        /
        <a href="./QingnanFan_files/cvpr_2018.bib">bibtex</a>
        <p></p>
        <p>The first demonstration of a single basic deep architecture capable of achieving state-of-the-art results when applied to each of the major intrinsic benchmarks. </p>
      </td>
    </tr>




    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/siga_2015.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/siga_2015.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
      <a href="https://www.dropbox.com/s/q2xvrag0g5l8wii/%5B2015%5D%5Bsiggraph_asia%5DJumpCut%20Non-Successive%20Mask%20Transfer%20and%20Interpolation%20for%20Video%20Cutout.pdf?dl=0">
            <papertitle>JumpCut: Non-Successive Mask Transfer and Interpolation for Video Cutout</papertitle>
      </a>
      <br>
            <strong>Qingnan Fan</strong>,
            <a href="http://vr.sdu.edu.cn/~zf/">Fan Zhong</a>,
            <a href="http://www.cs.huji.ac.il/~danix/">Dani Lischinski</a>,
            <a href="http://www.math.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
            <a href="http://www.cs.sdu.edu.cn/~baoquan/">Baoquan Chen</a>.
        <br>
        <em>SIGGRAPH Asia</em>, 2015 & <em>TOG</em>, 2015 <br>
        <a href="https://github.com/sduirc/JumpCut">codes</a>
        / 
        <a href="https://www.dropbox.com/scl/fi/jr84cmwryf50vnb61wb6e/siga_2015.pptx?dl=0&rlkey=9jn17dt75d0g2bi23wlax5hb6">slides</a>
        / 
        <a href="https://www.youtube.com/watch?v=drqnwDg0JFM&t=77s">video</a>
        / 
        <a href="https://www.dropbox.com/s/4b76crsifryn6rq/siga_2015_supp.rar?dl=0">supp file</a>
        / 
        <a href="https://www.dropbox.com/s/v0v3pkrhz1vizyt/VideoSeg_dataset.rar?dl=0">dataset</a>
        / 
        <a href="./QingnanFan_files/siga_2015.bib">bibtex</a>
        <!-- /
        <a href="http://irc.cs.sdu.edu.cn/JumpCut/">deprecated project page</a> -->
        <p></p>
        <p>An interactive real-time video segmentation algorithm. Significantly improve the video cutout accuracy and efficiency.</p>
      </td>
    </tr>


    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/a97-lu.gif" width="110" alt="" style="border-style: none" align="right">
        </div>
        <img src="./QingnanFan_files/a97-lu.gif" width="110" alt="" style="border-style: none" align="right">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
      <a href="https://www.dropbox.com/s/f84dqmtht41k8vv/a97-lu.pdf?dl=0">
            <papertitle>Build-to-Last: Strength to Weight 3D Printed Objects</papertitle>
      </a>
      <br>
            <a href="http://vr.sdu.edu.cn/~lulin/">Lin Lu</a>,
            <a href="http://www.cs.bgu.ac.il/~asharf/">Andrei Sharf</a>,
            Haisen Zhao, Yuan Wei,
            <strong>Qingnan Fan</strong>, Xuelin Chen,
            <a href="http://www.animlife.com/">Yann Savoye</a>,
            <a href="http://www.cs.sdu.edu.cn/zh/60">Changhe Tu</a>,
            <a href="http://www.math.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
            <a href="http://www.cs.sdu.edu.cn/~baoquan/">Baoquan Chen</a>.
        <br>
        <em>SIGGRAPH</em>, 2014 & <em>TOG</em>, 2014 <br>
        <a href="https://www.youtube.com/watch?v=V5IrPSvcm_8&t=5s">video</a>
        / 
        <a href="./QingnanFan_files/sig_2014.bib">bibtex</a>
        <p></p>
        <p>Reduce the material cost and weight of a given object while providing a durable printed model that is resistant to impact and external forces.</p>
      </td>
    </tr>

  </tbody></table>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td width="100%" valign="middle">
        <strong><heading>Work experience</heading></strong>
      </td>
    </tr>
    </tbody></table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="25%">
          <img src="./QingnanFan_files/vivo.jpg" width="110" alt="" style="border-style: none" align="right">
        </td>
        <td valign="top" width="75%">
          <papertitle>VIVO</papertitle>
          <br>
          Lead Researcher
          <br>
          2023-now
        </td>
      </tr>

      <tr>
        <td width="25%">
          <img src="./QingnanFan_files/tencent.png" width="110" alt="" style="border-style: none" align="right">
        </td>
        <td valign="top" width="75%">
          <papertitle>Tencent AI Lab</papertitle>
          <br>
          Senior Researcher
          <br>
          2021-2023
        </td>
      </tr>
  </table>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td width="100%" valign="middle">
        <strong><heading>Education</heading></strong>
      </td>
    </tr>
    </tbody></table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="25%">
          <img src="./QingnanFan_files/stanford.jpg" width="110" alt="" style="border-style: none" align="right">
        </td>
        <td valign="top" width="75%">
          <papertitle>Stanford University</papertitle>
          <br>
          Postdoctoral Researcher
          <br>
          Supervised by Prof. Leonidas Guibas
          <br>
          2019-2021
        </td>
      </tr>

      <tr>
        <td width="25%">
          <img src="./QingnanFan_files/SDU.jpg" width="110" alt="" style="border-style: none" align="right">
        </td>
        <td valign="top" width="75%">
          <papertitle>Shandong University</papertitle>
          <br>
          Ph.D. student
          <br>
          Supervised by Prof. Baoquan Chen
          <br>
          2014-2019
        </td>
      </tr>

      <tr>
        <td width="25%">
          <img src="./QingnanFan_files/SDU.jpg" width="110" alt="" style="border-style: none" align="right">
        </td>
        <td valign="top" width="75%">
          <papertitle>Shandong University</papertitle>
          <br>
          Undergraduate student
          <br>
          2010-2014
        </td>
      </tr>
  </table>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td width="100%" valign="middle">
        <strong><heading>Research experience</heading></strong>
      </td>
    </tr>
    </tbody></table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr>
      <td width="25%">
        <img src="./QingnanFan_files/bfa.jpg" width="110" alt="" style="border-style: none" align="right">
      </td>
      <td valign="top" width="75%">
        <papertitle>Beijing Film Academy</papertitle>
        <br>
        Research Intern
        <br>
        Supervised by Prof. Baoquan Chen
        <br>
        2018-2019
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src="./QingnanFan_files/cambridge.jpg" width="110" alt="" style="border-style: none" align="right">
      </td>
      <td valign="top" width="75%">
        <papertitle>University of Cambridge</papertitle>
        <br>
        Visiting Student
        <br>
        Supervised by Prof. Carola-Bibiane Schönlieb
        <br>
        2018
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src="./QingnanFan_files/msra.png" width="110" alt="" style="border-style: none" align="right">
      </td>
      <td valign="top" width="75%">
        <papertitle>Microsoft Research Asia</papertitle>
        <br>
        Research Intern
        <br>
        Supervised by Gang Hua, Xin Tong, Jiaolong Yang and David Wipf
        <br>
        2016-2018
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src="./QingnanFan_files/tau.jpg" width="110" alt="" style="border-style: none" align="right">
      </td>
      <td valign="top" width="75%">
        <papertitle>Tel Aviv University</papertitle>
        <br>
        Visiting Student
        <br>
        Supervised by Prof. Daniel Cohen-Or
        <br>
        2015
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src="./QingnanFan_files/The Hebrew University Jerusalem.png" width="110" alt="" style="border-style: none" align="right">
      </td>
      <td valign="top" width="75%">
        <papertitle>The Hebrew University of Jerusalem</papertitle>
        <br>
        Visiting Student
        <br>
        Supervised by Prof. Dani Lischinski
        <br>
        2014
      </td>
    </tr>
  </table>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td width="100%" valign="middle">
        <strong><heading>Talks</heading></strong>
        <p>APR. 2022: Active 3D scene understanding and its applications
          <br>“三维视觉与智能图形”前沿论坛, 图图名师讲堂, China
        </p>

        <p>OCT. 2021: Visual Localization
          <br>Embodied AI Workshop, Valse, China
        </p>

        <p>JAN. 2019: Deep Learning in Computational Photography
          <br>USC ICT/UW Reality Lab/Berkeley/Stanford/Google/MSR, US
        </p>  

        <p>DEC. 2018: Deep Learning for Single Image Artifact Removal
          <br>ACCV Tutorial 2018, Australia
        </p>

        <p>DEC. 2018: Image Smoothing via Unsupervised Learning
          <br>SIGGRAPH Asia 2018, Japan; GAMES Webinar, China
        </p>

        <p>AUG. 2018: Discovering Unsupervised Learning in Image Processing
          <br>CIA, Cambridge University, UK
        </p>

        <p>JUN. 2018: Revisiting Deep Intrinsic Image Decomposition
          <br>CVPR 2018, USA
        </p>

        <p>NOV. 2015: Interactive Real-time Video Segmentation
          <br>SIGGRAPH Asia 2015, Japan
        </p>
      </td>
    </tr>
    </tbody></table>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td width="100%" valign="middle">
        <strong><heading>Awards</heading></strong>
        <p>2022: Tencent Outstanding Contributor</p>
        <p>2020: CCF Doctorial Dissertation Award Nominee (CCF 优博提名)</p>
        <p>2019: Outstanding Academic Achievement Award of Shandong University</p>
        <p>2018: Academic Star Nominee of Shandong University (10/20000)</p>
        <p>2018: National Scholarship</p>
        <p>2016: Outstanding Academic Achievement Award of Shandong University</p>
        <p>2015: Presidential Scholarship of Shandong University (35/20000)
        (Highest honor for students in SDU, only 35 elected among around 20000 candidates)</p>
        <p>2015: National Scholarship</p>
        <p>2015: Pacemaker to Outstanding Graduate Student of Shandong University</p>
      </td>
    </tr>
    </tbody></table>

  

</body></html>