<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0023)https://jonbarron.info/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="“width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 24px;
    }
    subtitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 18px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="https://jonbarron.info/seal_icon.png">
  <title>Qingnan Fan (樊庆楠)</title>
  
  <link href="./QingnanFan_files/css" rel="stylesheet" type="text/css">
  <script charset="utf-8" src="chrome-extension://jgphnjokjhjlcnnajmfjlacjnjkhleah/js/btype.js"></script></head>

  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tbody><tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="100%" valign="middle">
        <p align="center">
          <name>Qingnan Fan (樊庆楠)</name>
        </p>
        <p>
          I am a Lead Researcher in the Imaging Algorithm Center of VIVO. 
          Our group is the core algorithm team responsible for advancing the photographic quality in the flagship smartphones with the cutting-edge technologies (3D, AIGC, etc).
        </p>
        <p>
          I was a Senior Researcher in the Visual Computing Center of Tencent AI Lab between 2021 to 2023.
        </p>
        <p>
          I was a Postdoctoral Researcher in Stanford University supervised by <a
              href="https://geometry.stanford.edu/member/guibas/"> Prof. Leonidas Guibas</a> between 2019 to 2021.
        </p>
        <p> I obtained my PhD degree in the  <a
                href="http://www.cs.sdu.edu.cn/">Computer Science and Technology School</a> of <a
                href="http://www.sdu.edu.cn/">Shandong University</a> at 2019. I was supervised by <a
                href="http://www.cs.sdu.edu.cn/~baoquan/"> Prof. Baoquan Chen</a>.
        </p>

        <p> <strong>If you are interested in the internship in our group for either publishing academic papers or landing the latest technologies, feel free to drop me an email.</strong> </p>

        <!-- <p>
            I was a research intern in the Visual Computing Group of MSRA supervised by <a href="http://www.davidwipf.com/">David Wipf</a> from Sept. 2016 to Feb. 2018. I also collaborated with <a href="https://www.microsoft.com/en-us/research/people/xtong/">Xin Tong</a>, <a href="https://www.microsoft.com/en-us/research/people/ganghua/">Gang Hua</a> and <a href="http://jlyang.org/">Jiaolong Yang</a> while in MSR.
        </p>
        <p>
            I was also a research intern in the Advanced Innovation Center for Future Visual Entertainment led by <a
                href="http://www.cs.sdu.edu.cn/~baoquan/"> Prof. Baoquan Chen</a>, in Beijing Film Academy between Mar. 2018 and Aug. 2019.
        </p>
        <p>
          I visited Tel Aviv University, Hebrew University of Jerusalem several times between 2014 to 2015 to work with 
              <a href="http://www.math.tau.ac.il/~dcor/">Prof. Daniel Cohen-Or</a> and 
              <a href="http://www.cs.huji.ac.il/~danix/">Prof. Dani Lischinski</a>.
        </p> -->
        <p align="center">
          <a href="mailto:fqnchina@gmail.com">Email</a> &nbsp/&nbsp
          <a href="./QingnanFan_files/qingnan_cv.pdf">CV</a> &nbsp/&nbsp
          <a href="./QingnanFan_files/Qingnan-bio.txt">Biography</a> &nbsp/&nbsp
          <a href="https://scholar.google.co.uk/citations?user=2cY2zwUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
          <a href="https://www.linkedin.com/in/qingnan-fan-02140aa7/">LinkedIn</a> &nbsp/&nbsp
          <a href="https://twitter.com/FanQingnan/">Twitter</a> &nbsp/&nbsp
          <a href="https://github.com/fqnchina/">Github</a>
        </p>
        </td>
        <td width="33%">
        <img src="./QingnanFan_files/qingnan_circle_v3.jpg" width="250" alt="" style="border-style: none" align="middle">
        </td>
      </tr>
      </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td width="100%" valign="middle">
            <strong><heading>Tech transfer</heading></strong>
            <p>
              <p>VIVO X200 series: <a href="https://www.vivo.com.cn/brand/news/detail?id=1273&type=0">AIGC for telephoto image enhancement (蓝图影像 - 大模型画质增强技术)</a></p>
            </p>
          </td>
        </tr>
        </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tbody><tr>
          <td width="100%" valign="middle">
            <strong><heading>BlueTalk</heading></strong>
            <p>
              I am the host of BlueTalk, which invites the expert speakers in the fields of computer vision, graphics and computational photography. The mission of BlueTalk is to promote the communication between academic and industrial communities, and explore the possibilities of landing the most innovative research ideas into any AI-powered industrial applications.
              If you are insterested in joining this family to give a talk, feel free to let me know.
              <p>2024-12-09: <a href="https://scholar.google.com.hk/citations?user=mmtNSisAAAAJ&hl=zh-CN">Du Chen</a>, PolyU, Real-world image super-resolution: solutions and challenges</p>
              <p>2024-10-30: <a href="https://www.mengweiren.com/">Mengwei Ren</a>, Adobe, Relightful harmonization: lighting-aware portrait background replacement</p>
              <p>2024-08-26: <a href="https://scholar.google.com/citations?user=kQUJjQQAAAAJ&hl=en">Jianqi Ma</a>, PolyU, AI vision research based on text</p>
              <p>2023-09-08: <a href="https://xinntao.github.io/">Xintao Wang</a>, Tencent, Visual generation and editing via diffusion models</p>
              <p>2023-08-15: <a href="http://www.cs.toronto.edu/~linghuan/">Huan Ling</a>, Nvidia, Generative models and vision perception using diffusion models</p>
              <p>2023-07-25: <a href="https://jianminbao.github.io/">Jianmin Bao</a>, MSRA, 2D & 3D visual synthesis and manipulation via diffusion models</p>
            </p>
          </td>
        </tr>
        </tbody></table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tbody><tr>
        <td width="100%" valign="middle">
          <strong><heading>Selected publications</heading></strong>
          <p>
            My research focus lies in computer graphics, 3D vision, image processing, and human-computer interaction. My recent effort has been spent on pushing the limit of 3D vision and reinforcement learning technologies to implement an intelligent embodied agent in both forms of physical robots and digital humans. <strong>For more recent research since 2024, please refer to my <a href="https://scholar.google.co.uk/citations?user=2cY2zwUAAAAJ&hl=en">google scholar page</a>.</strong>
          </p>
        </td>
      </tr>
      </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id="portrait_image" style="opacity: 0;">
            <img src="./QingnanFan_files/siga_2023_activity_small.gif" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <img src="./QingnanFan_files/siga_2023_activity_small.gif" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <script type="text/javascript">
            function portrait_start() {
            document.getElementById('portrait_image').style.opacity = "1";
            }
            function portrait_stop() {
            document.getElementById('portrait_image').style.opacity = "0";
            }
            portrait_stop()
            </script>
          </td>
          <td valign="top" width="75%">
            <!-- <a href="./QingnanFan_files/siggraphasia_2023.pdf"></a> -->
            <papertitle>Scene-aware Activity Program Generation with Language Guidance</papertitle>
        <br>     
        Zejia Su,
        <strong>Qingnan Fan</strong>, 
        <a href="https://xuelin-chen.github.io/">Xuelin Chen</a>, 
        <a href="https://people.scs.carleton.ca/~olivervankaick/">Oliver van Kaick</a>, 
        <a href="https://vcc.tech/~huihuang">Hui Huang</a>, 
        <a href="https://csse.szu.edu.cn/staff/ruizhenhu/">Ruizhen Hu</a>.
        <br>
            <em>SIGGRAPH Asia</em>, 2023 & TOG, 2023</strong><br>
            <a href="https://toddbear.github.io/LangGuidedProg/" target="_blank">project page</a>
            /
            <a href="./QingnanFan_files/tog_2023_supp.pdf" target="_blank">supp file</a>
            /
            <a href="./QingnanFan_files/tog_2023.bib">bibtex</a>
            <p></p>
            <p> We address the problem of scene-aware activity program generation, which requires decomposing a given activity task into instructions that can be sequentially performed within a target scene to complete the activity.</p>
          </td>
        </tr>


        <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
          <td width="25%">
            <div class="one">
            <div class="two" id="portrait_image" style="opacity: 0;">
            <img src="./QingnanFan_files/siga_2023_ease.gif" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <img src="./QingnanFan_files/siga_2023_ease.gif" width="175" alt="" style="border-style: none" align="middle">
            </div>
            <script type="text/javascript">
            function portrait_start() {
            document.getElementById('portrait_image').style.opacity = "1";
            }
            function portrait_stop() {
            document.getElementById('portrait_image').style.opacity = "0";
            }
            portrait_stop()
            </script>
          </td>
          <td valign="top" width="75%">
            <a href="./QingnanFan_files/siga_2023.pdf">
              <papertitle>C·ASE: Learning Conditional Adversarial Skill Embeddings for Physics-based Characters</papertitle>
            </a>
        <br>     
        <a href="https://frank-zy-dou.github.io/">Zhiyang Dou</a>,
        <a href="https://xuelin-chen.github.io/">Xuelin Chen</a>, 
        <strong>Qingnan Fan</strong>, 
        <a href="https://homepages.inf.ed.ac.uk/tkomura/">Taku Komura</a>, 
        <a href="https://engineering.tamu.edu/cse/profiles/Wang-Wenping.html">Wenping Wang</a>.
        <br>
            <em>SIGGRAPH Asia</em>, 2023</strong><br>
            <a href="http://arxiv.org/abs/2309.11351" target="_blank">arXiv</a>
            /
            <a href="https://frank-zy-dou.github.io/projects/CASE/index.html" target="_blank">project page</a>
            /
            <a href="https://youtu.be/Cgq6JbQ1VW4" target="_blank">video</a>
            /
            <a href="./QingnanFan_files/siga_2023.bib">bibtex</a>
            <p></p>
            <p> We present C·ASE, an efficient and effective framework that learns conditional Adversarial Skill Embeddings for Elite physics-based characters.</p>
          </td>
        </tr>
   
        
    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/cvpr_2023.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/cvpr_2023.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/cvpr_2023.pdf">
            <papertitle>3D-Aware Object Goal Navigation via Simultaneous Exploration and Identification</papertitle>
    </a>
    <br>     
    <a href="https://jzhzhang.github.io/">Jiazhao Zhang*</a>,
    <a href="https://liudai-homepage.github.io/">Liu Dai*</a>, 
    <a href="https://mfp0610.github.io/">Fanpeng Meng</a>, 
    <strong>Qingnan Fan</strong>, 
    <a href="https://xuelin-chen.github.io/">Xuelin Chen</a>, 
    <a href="https://kevinkaixu.net/">Kai Xu</a>, 
    <a href="https://hughw19.github.io/">He Wang</a>.
    <br>
        <em>CVPR</em>, 2023</strong><br>
        <a href="https://arxiv.org/abs/2212.00338" target="_blank">arXiv</a>
        /
        <a href="https://pku-epic.github.io/3D-Aware-ObjectNav/" target="_blank">project page</a>
        /
        <a href="./QingnanFan_files/cvpr_2023.bib">bibtex</a>
        <p></p>
        <p> We propose a framework for the challenging 3D-aware Object goal navigation task based on two straightforward sub-policies. The two sub-polices, namely
          corner-guided exploration policy and category-aware identification policy, simultaneously perform by utilizing online fused 3D points as observation.</p>
      </td>
    </tr>


    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/iclr_2023.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/iclr_2023.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/iclr_2023.pdf">
            <papertitle>DualAfford: Learning Collaborative Visual Affordance for Dual-gripper Object Manipulation</papertitle>
    </a>
    <br>     
    <a href="https://www.researchgate.net/profile/Yan-Zhao-182" target="_blank">Yan Zhao*</a>,
    <a href="https://warshallrho.github.io/" target="_blank">Ruihai Wu*</a>,
    <a href="https://acmlczh.github.io/">Zhehuan Chen</a>, 
    Yourong Zhang,
    <strong>Qingnan Fan</strong>, 
    <a href="https://cs.stanford.edu/~kaichun/">Kaichun Mo</a>, 
    <a href="https://zsdonghao.github.io/" target='_blank'>Hao Dong</a>.
    <br>
        <em>ICLR</em>, 2023</strong><br>
        <a href="https://arxiv.org/abs/2207.01971" target="_blank">arXiv</a>
        /
        <a href="https://hyperplane-lab.github.io/DualAfford/" target="_blank">project page</a>
        /
        <a href="https://www.youtube.com/watch?v=3NsnIIrgv0w" target="_blank">video</a>
        /
        <a href="./QingnanFan_files/iclr-2023.bib">bibtex</a>
        <p></p>
        <p> We propose a novel learning framework, DualAfford, to learn collaborative affordance for dual-gripper manipulation tasks. The core design of the approach is
          to reduce the quadratic problem for two grippers into two disentangled yet interconnected subtasks for efficient learning.</p>
      </td>
    </tr>


    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/eccv_2022_localization.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/eccv_2022_localization.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/eccv_2022_localization.pdf">
      <papertitle>Towards Accurate Active Camera Localization</papertitle>
    </a>
    <br>     
    Qihang Fang*, 
    <a href="https://yd-yin.github.io/">Yingda Yin*</a>, 
    <strong>Qingnan Fan</strong>, 
    <a href="https://fxia22.github.io/">Fei Xia</a>,
    <a href="https://scholar.google.com/citations?user=vtZMhssAAAAJ&hl=en/">Siyan Dong</a>, 
    Sheng Wang,
    <a href="https://juewang725.github.io/">Jue Wang</a>,
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas Guibas</a>,
    <a href="https://cfcs.pku.edu.cn/baoquan/">Baoquan Chen</a>.
    <br>
        <em>ECCV</em>, 2022</strong><br>
        <a href="https://arxiv.org/abs/2012.04263" target="_blank">arXiv</a>
        /
        <a href="https://github.com/qhFang/AccurateACL">codes</a>
        /
        <a href="https://www.youtube.com/watch?v=pDMoZ6pjkkQ">video</a>
        /
        <a href="./QingnanFan_files/eccv_2022_localization_supp.pdf">supp file</a>
        /
        <a href="./QingnanFan_files/eccv_2022_localization.bib">bibtex</a>
        <p></p>
        <p> In this work, we explicitly model the camera and scene uncertainty components to solve the problem of active camera localization by reinforcement learning. Our algorithm improves over the state-of-the-art Markov Localization based approaches by a large margin on the fine-scale camera pose accuracy.</p>
      </td>
    </tr>


    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/eccv_2022_AdaAfford.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/eccv_2022_AdaAfford.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/eccv_2022_AdaAfford.pdf">
      <papertitle>AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated Objects via Few-shot Interactions</papertitle>
    </a>
    <br>     
    <a href="https://github.com/galaxy-qazzz">Yian Wang*</a>, 
    <a href="https://warshallrho.github.io/" target="_blank">Ruihai Wu*</a>,
    <a href="https://cs.stanford.edu/~kaichun/">Kaichun Mo*</a>, 
    Jiaqi Ke,
    <strong>Qingnan Fan</strong>, 
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas Guibas</a>,
    <a href="https://zsdonghao.github.io/" target='_blank'>Hao Dong</a>.
    <br>
        <em>ECCV</em>, 2022</strong><br>
        <a href="https://arxiv.org/abs/2112.00246" target="_blank">arXiv</a>
        /
        <a href="https://hyperplane-lab.github.io/AdaAfford/" target="_blank">project page</a>
        /
        <a href="https://github.com/wangyian-me/AdaAffordCode/" target="_blank">codes</a>
        /
        <a href="https://www.youtube.com/watch?v=Qaq0YkpNxe4">video</a>
        /
        <a href="./QingnanFan_files/eccv_2022_AdaAfford.bib">bibtex</a>
        <p></p>
        <p> In this paper, we propose a novel framework, named AdaAfford, that learns to perform very few test-time interactions for quickly adapting the affordance priors to more accurate instance-specific posteriors.</p>
      </td>
    </tr>


    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/cvpr22_mapping.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/cvpr22_mapping.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/cvpr_2022_mapping.pdf">
      <papertitle>Multi-Robot Active Mapping via Neural Bipartite Graph Matching</papertitle>
    </a>
    <br>     
    Kai Ye*, 
    <a href="https://scholar.google.com/citations?user=vtZMhssAAAAJ&hl=en/">Siyan Dong*</a>, 
    <strong>Qingnan Fan</strong>, 
    <a href="https://hughw19.github.io/">He Wang</a>, 
    <a href="https://ericyi.github.io/">Li Yi</a>, 
    <a href="https://fxia22.github.io/">Fei Xia</a>,
    <a href="https://juewang725.github.io/">Jue Wang</a>,
    <a href="https://cfcs.pku.edu.cn/baoquan/">Baoquan Chen</a>.
    <br>
        <em>CVPR</em>, 2022</strong><br>
        <a href="https://arxiv.org/abs/2203.16319" target="_blank">arXiv</a>
        /
        <a href="https://github.com/siyandong/NeuralCoMapping" target="_blank">codes</a>
        /
        <a href="https://www.youtube.com/watch?v=M7LRzWDG6mk" target="_blank">video</a>
        /
        <a href="./QingnanFan_files/cvpr_2022_mapping_supp.pdf">supp file</a>
        /
        <a href="./QingnanFan_files/cvpr_2022_mapping_poster.pdf">poster</a>
        /
        <a href="./QingnanFan_files/cvpr_2022_mapping.bib">bibtex</a>
        <p></p>
        <p> We propose a novel multi-robot active mapping algorithm by reducing the problem to bipartite graph matching, solved by the proposed multiplex graph neural network (mGNN) via reinforcement learning.</p>
      </td>
    </tr>


    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/cvpr22_adela.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/cvpr22_adela.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/cvpr_2022_adela.pdf">
          <papertitle>ADeLA: Automatic Dense Labeling with Attention for Viewpoint Shift in Semantic Segmentation</papertitle>
    </a>
    <br>     
    <a href="https://yanchaoyang.github.io/">Yanchao Yang*</a>, 
    Hanxiang Ren*, 
    <a href="https://hughw19.github.io/">He Wang</a>, 
    <a href="https://cs.stanford.edu/people/bshen88/">Bokui Shen</a>, 
    <strong>Qingnan Fan</strong>, 
    <a href="https://www.youyizheng.net/">Youyi Zheng</a>, 
    <a href="https://ckllab.stanford.edu/">C. Karen Liu</a>,
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas Guibas</a>.
    <br>
        <em>CVPR</em>, 2022 <strong style="color:red">(Oral)</strong><br>
        <a href="https://arxiv.org/abs/2107.14285" target="_blank">arXiv</a>
        /
        <a href="./QingnanFan_files/cvpr_2022_adela.bib">bibtex</a>
        <p></p>
        <p> We describe a method to deal with performance drop in semantic segmentation caused by viewpoint changes within multi-camera systems, where temporally paired images are readily available, but the annotations may only be abundant for a few typical views.</p>
      </td>
    </tr>


    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/vat_mart.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/vat_mart.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/iclr_2022.pdf">
            <papertitle>VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects</papertitle>
    </a>
    <br>     
    <a href="https://warshallrho.github.io/" target="_blank">Ruihai Wu*</a>,
    <a href="https://www.researchgate.net/profile/Yan-Zhao-182" target="_blank">Yan Zhao*</a>,
    <a href="https://cs.stanford.edu/~kaichun/">Kaichun Mo*</a>, 
    <a href="https://guozz.cn/" target='_blank'>Zizheng Guo</a>,
    <a href="https://github.com/galaxy-qazzz" target='_blank'>Yian Wang</a>,
    <a href="https://tianhaowuhz.github.io/" target="_blank">Tianhao Wu</a>,
    <strong>Qingnan Fan</strong>, 
    <a href="https://xuelin-chen.github.io/" target='_blank'>Xuelin Chen</a>,
    <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas Guibas</a>,
    <a href="https://zsdonghao.github.io/" target='_blank'>Hao Dong</a>.
    <br>
        <em>ICLR</em>, 2022</strong><br>
        <a href="https://arxiv.org/abs/2106.14440" target="_blank">arXiv</a>
        /
        <a href="https://hyperplane-lab.github.io/vat-mart" target="_blank">project page</a>
        /
        <a href="https://github.com/warshallrho/VAT-Mart" target="_blank">codes</a>
        /
        <a href="https://www.youtube.com/watch?v=HjhsLKf1eQY" target="_blank">video</a>
        /
        <a href="./QingnanFan_files/iclr-2022.bib">bibtex</a>
        <p></p>
        <p> We design an interaction-for-perception framework, VAT-MART, to learn actionable visual representations for more effective manipulation of 3D articulated objects.</p>
      </td>
    </tr>

    
    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/iccv_2021_captra3.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/iccv_2021_captra3.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/iccv_2021_captra.pdf">
            <papertitle>CAPTRA: CAtegory-level Pose Tracking for Rigid and Articulated Objects from Point Clouds</papertitle>
    </a>
    <br>          
    Yijia Weng*, 
    <a href="https://hughw19.github.io/">He Wang*</a>, 
    Qiang Zhou,
    <a href="https://yzqin.github.io/">Yuzhe Qin</a>, 
    <a href="https://geometry.stanford.edu/person.php?id=duanyq19">Yueqi Duan</a>, 
    <strong>Qingnan Fan</strong>, 
    <a href="https://cfcs.pku.edu.cn/baoquan/">Baoquan Chen</a>,
    <a href="http://ai.ucsd.edu/~haosu/">Hao Su</a>,
    <a href="https://geometry.stanford.edu/member/guibas/index.html">Leonidas Guibas</a>.
    <br>
        <em>ICCV</em>, 2021 <strong style="color:red">(Oral)</strong><br>
        <a href="https://arxiv.org/abs/2104.03437">arXiv</a>
        /
        <a href="https://yijiaweng.github.io/CAPTRA/" target="_blank">project page</a>
        /
        <a href="https://github.com/HalfSummer11/CAPTRA">codes</a>
        /
        <a href="https://www.youtube.com/watch?v=JFPcOHCH2O0">video</a>
        /
        <a href="./QingnanFan_files/iccv_2021_captra.bib">bibtex</a>
        <p></p>
        <p> For the first time, we propose a unified framework that can handle 9-DoF pose tracking for novel rigid object instances as well as per-part pose tracking for 3D articulated objects.</p>
      </td>
    </tr>


    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/iccv_2021_tupleinfoNCE_large.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/iccv_2021_tupleinfoNCE_large.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/iccv_2021_tupleinfoNCE.pdf">
            <papertitle>Contrastive Multimodal Fusion with TupleInfoNCE</papertitle>
    </a>
    <br>          
    Yunze Liu,
    <strong>Qingnan Fan</strong>, 
    <a href="https://www.shanghangzhang.com/">Shanghang Zhang</a>, 
    <a href="https://zsdonghao.github.io/">Hao Dong</a>,
    <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>,
    <a href="https://ericyi.github.io/">Li Yi</a>.
    <br>
        <em>ICCV</em>, 2021</strong><br>
        <a href="https://arxiv.org/abs/2107.02575">arXiv</a>
        /
        <a href="./QingnanFan_files/iccv_2021_tupleinfoNCE.bib">bibtex</a>
        <p></p>
        <p> We propose a novel contrastive learning objective, TupleInfoNCE. It contrasts tuples based not only on positive and negative correspondences, but also by composing new negative tuples using modalities describing different scenes.</p>
      </td>
    </tr>


    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/cvpr21_localization_thumbnail.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/cvpr21_localization_thumbnail.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/cvpr_2021_localization.pdf">
            <papertitle>Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments</papertitle>
    </a>
    <br>          
    <a href="https://scholar.google.com/citations?user=vtZMhssAAAAJ&hl=en/">Siyan Dong*</a>, 
    <strong>Qingnan Fan*</strong>, 
    <a href="https://hughw19.github.io/">He Wang</a>, 
    Ji Shi,
    <a href="https://ericyi.github.io/">Li Yi</a>, 
    <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>,
    <a href="https://cfcs.pku.edu.cn/baoquan/">Baoquan Chen</a>,
    <a href="https://geometry.stanford.edu/member/guibas/index.html">Leonidas Guibas</a>.
    <br>
        <em>CVPR</em>, 2021 <strong style="color:red">(Oral)</strong><br>
        <a href="https://arxiv.org/abs/2012.04746">arXiv</a>
        /
        <a href="https://github.com/siyandong/NeuralRouting">codes</a>
        /
        <a href="https://www.youtube.com/watch?v=_1eInWKbuVA" target="_blank">video</a>
        /
        <a href="./QingnanFan_files/cvpr_2021_localization.bib">bibtex</a>
        <p></p>
        <p> A novel outlier-aware neural tree to tackle the camera localization challenges in dynamic indoor environments. It achieves the best performance in the RIO-10 benchmark.</p>
      </td>
    </tr>

    
    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/cvpr21_manga_thumbnailv2.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/cvpr21_manga_thumbnailv2.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="https://www.dropbox.com/s/33mbcwfv3e4n1v7/cvpr21_manga.pdf?dl=0">
            <papertitle>Generating Manga from Illustrations via Mimicking Manga Creation Workflow</papertitle>
    </a>
    <br>          
    <a href="https://github.com/lllyasviel">Lvmin Zhang</a>, 
    <a href="https://github.com/SystemErrorWang">Xinrui Wang</a>, 
    <strong>Qingnan Fan</strong>, 
    Yi Ji, 
    ChunPing Liu.
    <br>
        <em>CVPR</em>, 2021<br>
        <a href="https://lllyasviel.github.io/MangaFilter/">project page</a>
        / 
        <a href="./QingnanFan_files/cvpr_2021_manga.bib">bibtex</a>
        <p></p>
        <p> A data-driven framework to convert a digital illustration into three corresponding components: manga line drawing, regular screentone, and irregular screen texture. These components can be directly composed into manga images and can be further retouched for more plentiful manga creations.</p>
      </td>
    </tr>


      <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
        <td width="25%">
          <div class="one">
          <div class="two" id="portrait_image" style="opacity: 0;">
          <img src="./QingnanFan_files/tpami21_thumbnail.gif" width="175" alt="" style="border-style: none" align="middle">
          </div>
          <img src="./QingnanFan_files/tpami21_thumbnail.gif" width="175" alt="" style="border-style: none" align="middle">
          </div>
          <script type="text/javascript">
          function portrait_start() {
          document.getElementById('portrait_image').style.opacity = "1";
          }
          function portrait_stop() {
          document.getElementById('portrait_image').style.opacity = "0";
          }
          portrait_stop()
          </script>
        </td>
        <td valign="top" width="75%">
      <a href="./QingnanFan_files/tpami_2021.pdf">
              <papertitle>A General Decoupled Learning Framework for Parameterized Image Operators</papertitle>
      </a>
      <br>
      <strong>Qingnan Fan*</strong>, 
      <a href="http://www.dongdongchen.bid/">Dongdong Chen*</a>, 
      <a href="http://www.lyuan.org/">Lu Yuan</a>,
      <a href="https://www.microsoft.com/en-us/research/people/ganghua/">Gang Hua</a>, 
      <a href="http://staff.ustc.edu.cn/~ynh/">Nenghai Yu</a>, 
      <a href="http://www.cs.sdu.edu.cn/~baoquan/">Baoquan Chen</a>.
      <br>
          <em>TPAMI</em>, 2021 <br>
          <a href="https://arxiv.org/abs/1907.05852">arXiv</a>
          / 
          <a href="https://github.com/fqnchina/DecoupleLearning">codes</a>
          /
          <!-- <a href="./QingnanFan_files/tpami21_video-presentation.mp4">demo</a>
          / -->
          <a href="./QingnanFan_files/tpami_2021.bib">bibtex</a>
          <p></p>
          <p> A journal extension of our ECCV 2018 paper. We further propose a cheap parameter-tuning version of the decouple learning framework that enables real-time alternation between different image operators. </p>
        </td>
      </tr>


    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/nips_2020_v2.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/nips_2020_v2.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/nips_2020_part_assembly.pdf">
            <papertitle>Generative 3D Part Assembly via Dynamic Graph Learning</papertitle>
    </a>
    <br>
    Jialei Huang*,
    <a href="https://championchess.github.io/">Guanqi Zhan*</a>, 
    <strong>Qingnan Fan</strong>, 
    <a href="https://cs.stanford.edu/~kaichun/">Kaichun Mo</a>, 
    <a href="https://linsats.github.io/">Lin Shao</a>,
    <a href="https://cfcs.pku.edu.cn/baoquan/">Baoquan Chen</a>, 
    <a href="https://geometry.stanford.edu/member/guibas/index.html">Leonidas Guibas</a>, 
    <a href="https://zsdonghao.github.io/">Hao Dong</a>.
    <br>
        <em>NeurIPS</em>, 2020 <br>
        <a href="https://arxiv.org/abs/2006.07793">arXiv</a>
        / 
        <a href="https://hyperplane-lab.github.io/Generative-3D-Part-Assembly/">project page</a>
        / 
        <a href="https://github.com/Championchess/Generative-3D-Part-Assembly">codes</a>
        / 
        <a href="./QingnanFan_files/nips_2020_part_assembly.bib">bibtex</a>
        / 
        press (<a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650799676&idx=4&sn=1d2c6761bd3892686aacfc5d5dc1c14c&chksm=871a3a42b06db35402bc4e3ed6334f99b0e408ccf9ea30a072ec8bbd4d510e2a35887b563408&scene=0&xtrack=1&key=b21c8bfc2e98d5d832fabcf5a6128c64545a73289c8282c226af065f0e8e665138087c317b186eaebc3355bd9f2e7898b264453a14bd40dffbbb6fef2ef4abeec2a55a98cd3059f89720fff44355005b3bd6c07b912f9076963bed81b37c096beb0b826a868249f36912f97e3e7aaddf2db31632acfed190f0cc958aac41e3d8&ascene=1&uin=NDE4NjY0ODU1&devicetype=Windows+10+x64&version=6300002f&lang=zh_CN&exportkey=AWEFijHWuK4REu4n0VpJCOc%3D&pass_ticket=%2FxhWQurVrRrCi87xYVsAtep82meBarP1vauCZheyCMo17a9a4Hpo24ulX3EPdonG&wx_header=0">机器之心</a>,<a href="https://mp.weixin.qq.com/s?__biz=MzA5ODEzMjIyMA==&mid=2247547123&idx=4&sn=dea62a5edc7a65687d08872ce7ebb5ac&chksm=90943160a7e3b876a68ad4b69af47d5b57e40d50d5ba0034db79e2801dc6d6531eacee36e635&mpshare=1&scene=1&srcid=1017FVRyXZjpjkBieLaw7eQX&sharer_sharetime=1602869239113&sharer_shareid=49c5a30c6732bec1bffca4ce2cfd738b&key=905d9831417cd6b735a634c11fe9e11a86e7bec0fce71b3b6a37c07ef6c6049aea49aff5f0223327ab828a3657dbcd0fb4b9aad0c19b9ee4dedc401426ecd5fc0a44f846a5fb951c4039c3b26e3184934d8810e75e3a13d21762ae46ec85e060c0e769031a118bb4fcaf7d0d6d6c8944979019e20273ca05103f170d592abb78&ascene=1&uin=NDE4NjY0ODU1&devicetype=Windows+10+x64&version=6300002f&lang=zh_CN&exportkey=AYpfmqiYvoArYHlAv2MUXGs%3D&pass_ticket=%2FxhWQurVrRrCi87xYVsAtep82meBarP1vauCZheyCMo17a9a4Hpo24ulX3EPdonG&wx_header=0">AI科技评论</a>)
        <p></p>
        <p> A dynamic graph learning algorithm for autonomous part assembly. It learns to reason an assembly-oriented dynamically-evolved relation graph, which indicates the assembly process which is guided by the major parts (chair leg&seat).</p>
      </td>
    </tr>


    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/tip20_thumbnailv3.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/tip20_thumbnailv3.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/tip-2020.pdf">
            <papertitle>Controllable Image Processing via Adaptive FilterBank Pyramid</papertitle>
    </a>
    <br>
    <a href="http://www.dongdongchen.bid/">Dongdong Chen</a>, 
    <strong>Qingnan Fan</strong>,   
    <a href="https://liaojing.github.io/html/">Jing Liao</a>,
    <a href="https://angelicaiaviles.wordpress.com/">Angelica I. Aviles-Rivero</a>, 
    <a href="http://www.lyuan.org/">Lu Yuan</a>,
    <a href="http://staff.ustc.edu.cn/~ynh/">Nenghai Yu</a>, 
    <a href="https://www.microsoft.com/en-us/research/people/ganghua/">Gang Hua</a>.
    <br>
        <em>TIP</em>, 2020 <br>
        <a href="./QingnanFan_files/tip-2020.bib">bibtex</a>
        <p></p>
        <p> we propose a new plugin module, “Adaptive Filterbank Pyramid”, which can be inserted into a backbone network to support multiple operators and continuous parameter tuning.</p>
      </td>
    </tr>


    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/iccv19_thumbnail_v3.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/iccv19_thumbnail_v3.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/iccv_2019.pdf">
            <papertitle>RainFlow: Optical Flow under Rain Streaks and Rain Veiling Effect</papertitle>
    </a>
    <br>    
    <a href="https://liruoteng.github.io/">Ruoteng Li</a>, 
    <a href="http://tanrobby.github.io/">Robby T. Tan</a>, 
    <a href="https://www.ece.nus.edu.sg/stfpage/eleclf/">Loong-Fah Cheong</a>,
    <a href="https://angelicaiaviles.wordpress.com/">Angelica I. Aviles-Rivero</a>, 
    <strong>Qingnan Fan</strong>, 
    <a href="http://www.damtp.cam.ac.uk/user/cbs31/Home.html">Carola-Bibiane Schönlieb</a>.
    <br>
        <em>ICCV</em>, 2019 <br>
        <a href="./QingnanFan_files/iccv_2019.bib">bibtex</a>
        <p></p>
        <p> A deep-learning based optical flow approach designed to handle heavy rain. </p>
      </td>
    </tr>


    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/miccai2019_thumbnail.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/miccai2019_thumbnail.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <!-- <papertitle>GraphX<sup>NET</sup> - Chest X-Ray Classification Under Extreme Minimal Supervision</papertitle> -->
    <a href="./QingnanFan_files/miccai_2019.pdf">
            <papertitle>GraphX<sup>NET</sup> - Chest X-Ray Classification Under Extreme Minimal Supervision</papertitle>
    </a>
    <br>
    <a href="https://angelicaiaviles.wordpress.com/">Angelica Aviles-Rivero</a>,
    <a href="https://www.math.u-bordeaux.fr/~npapadak/">Nicolas Papadakis</a>, 
    <a href="https://liruoteng.github.io/">Ruoteng Li</a>, 
    <a href="https://www.ccimi.maths.cam.ac.uk/members/profile/Philip%20Sellars/">Philip Sellars</a>, 
    <strong>Qingnan Fan</strong>, 
    <a href="https://tanrobby.github.io/">Robby Tan</a>,
    <a href="http://www.damtp.cam.ac.uk/user/cbs31/Home.html">Carola-Bibiane Schönlieb</a>.
    
    <br>
        <em>MICCAI</em>, 2019 <br>
        <a href="https://arxiv.org/abs/1907.10085">arXiv</a>
        / 
        <a href="./QingnanFan_files/miccai_2019.bib">bibtex</a>
        <p></p>
        <p> A novel semi-supervised framework for X-ray classification which is based on a graph-based optimisation model. A new multi-class classification functional that strengthens the synergy between the limited number of labels and the huge amount of unlabelled data. </p>
      </td>
    </tr>


    <tr onmouseout="tip19_stop()" onmouseover="tip19_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="tip19_thumbnail" style="opacity: 0;">
        <img src="./QingnanFan_files/tip19_thumbnail3.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/tip19_thumbnail3.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function tip19_start() {
        document.getElementById('tip19_thumbnail').style.opacity = "1";
        }
        function tip19_stop() {
        document.getElementById('tip19_thumbnail').style.opacity = "0";
        }
        tip19_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/tip_2019.pdf">
            <papertitle>Mirror, Mirror, on the Wall, Who's Got the Clearest Image of Them All? - A Tailored Approach to Single Image Reflection Removal</papertitle>
    </a>
    <br>
    <a href="https://danielheydecker.wordpress.com/">Daniel Heydecker*</a>, 
    <a href="http://www.damtp.cam.ac.uk/user/gam37/">Georg Maierhofer*</a>, 
    <a href="https://angelicaiaviles.wordpress.com/">Angelica Aviles-Rivero*</a>,
    <strong>Qingnan Fan</strong>, 
    <a href="http://www.dongdongchen.bid/">Dongdong Chen</a>,
    <a href="http://www.damtp.cam.ac.uk/user/cbs31/Home.html">Carola-Bibiane Schönlieb</a>,
    <a href="https://ivrl.epfl.ch/people/people-susstrunk/">Sabine Süsstrunk</a>.
    <br>
        <em>TIP</em>, 2019 <br>
        <a href="https://arxiv.org/abs/1805.11589">arXiv</a>
        / 
        <a href="./QingnanFan_files/tip_2019.bib">bibtex</a>
        <p></p>
        <p> A simple and tractable user interactive tool for single image reflection removal, which is facilitated with a spatially-aware prior term solved by an efficient half-quadratic splitting optimization approach. </p>
      </td>
    </tr>


    <tr onmouseout="wacv19_stop()" onmouseover="wacv19_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="wacv19_thumbnail" style="opacity: 0;">
          <img src="./QingnanFan_files/wacv19_thumbnail_before.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/wacv19_thumbnail_after.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function wacv19_start() {
        document.getElementById('wacv19_thumbnail').style.opacity = "1";
        }
        function wacv19_stop() {
        document.getElementById('wacv19_thumbnail').style.opacity = "0";
        }
        wacv19_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/wacv_2019.pdf">
            <papertitle>Gated Context Aggregation Network for Image Dehazing and Deraining</papertitle>
    </a>
    <!-- <papertitle>Image Smoothing via Unsupervised Learning</papertitle> -->
    <br>
    <a href="http://www.dongdongchen.bid/">Dongdong Chen</a>, 
    <a href="http://mingminghe.com/">Mingming He</a>, 
    <strong>Qingnan Fan</strong>, 
    <a href="https://liaojing.github.io/html/">Jing Liao</a>,
    <a href="https://scholar.google.com/citations?user=vUs1ptEAAAAJ&hl=en">Liheng Zhang</a>,
    <a href="http://home.ustc.edu.cn/~houdd/">Dongdong Hou</a>,
    <a href="http://www.lyuan.org/">Lu Yuan</a>,
    <a href="https://www.microsoft.com/en-us/research/people/ganghua/">Gang Hua</a>.

    <br>
        <em>WACV</em>, 2019 <br>
        <a href="https://arxiv.org/abs/1811.08747">arXiv</a>
        / 
        <a href="https://github.com/cddlyf/GCANet">codes</a>
        / 
        <a href="./QingnanFan_files/wacv_2019.bib">bibtex</a>
        <p></p>
        <p> A novel end-to-end gated context aggregation network GCANet that outperforms all the existing appraoches by a large margin on both image dehazing and deraining tasks.</p>
      </td>
    </tr>


    <tr onmouseout="siga18_stop()" onmouseover="siga18_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="siga18_thumbnail" style="opacity: 0;">
          <img src="./QingnanFan_files/siga18_thumbnail_before.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/siga18_thumbnail_after.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function siga18_start() {
        document.getElementById('siga18_thumbnail').style.opacity = "1";
        }
        function siga18_stop() {
        document.getElementById('siga18_thumbnail').style.opacity = "0";
        }
        siga18_stop()
        </script>
      </td>
      <td valign="top" width="75%">
    <a href="./QingnanFan_files/siggraph_asia_2018.pdf">
            <papertitle>Image Smoothing via Unsupervised Learning</papertitle>
    </a>
    <!-- <papertitle>Image Smoothing via Unsupervised Learning</papertitle> -->
    <br>
        <strong>Qingnan Fan</strong>, 
        <a href="http://jlyang.org/">Jiaolong Yang</a>, 
        <a href="http://www.davidwipf.com/">David Wipf</a>,        
        <a href="http://www.cs.sdu.edu.cn/~baoquan/">Baoquan Chen</a>,
        <a href="https://www.microsoft.com/en-us/research/people/xtong/">Xin Tong</a>.
    <br>
        <em>SIGGRAPH Asia</em>, 2018 & <em>TOG</em>, 2018 <br>
        <a href="https://arxiv.org/abs/1811.02804">arXiv</a>
        / 
        <a href="https://github.com/fqnchina/ImageSmoothing">codes</a>
        / 
        <a href="https://www.dropbox.com/s/p3ql7etstto1g4e/siggraph_asia_2018_supp.pdf?dl=0">supp file</a>
        /
        <a href="./QingnanFan_files/siga_2018.bib">bibtex</a>
        <p></p>
        <p> Treat deep learning as an optimization tool to minimize the proposed image smoothing objective function in an unsupervised manner. Multiple different smoothing effects can be easily learned by adaptively changing the proposed objective function.</p>
      </td>
    </tr>


    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
          <img src="./QingnanFan_files/eccv18_thumbnail.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/eccv18_thumbnail.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
        <a href="./QingnanFan_files/eccv_2018.pdf">
                <papertitle>Decouple Learning for Parameterized Image Operators</papertitle>
        </a>
        <br>
        <strong>Qingnan Fan*</strong>, 
        <a href="http://www.dongdongchen.bid/">Dongdong Chen*</a>, 
        <a href="http://www.lyuan.org/">Lu Yuan</a>,
        <a href="https://www.microsoft.com/en-us/research/people/ganghua/">Gang Hua</a>, 
        <a href="http://staff.ustc.edu.cn/~ynh/">Nenghai Yu</a>, 
        <a href="http://www.cs.sdu.edu.cn/~baoquan/">Baoquan Chen</a>.
        <br>
        <em>ECCV</em>, 2018 <br>
        <a href="https://arxiv.org/abs/1807.08186">arXiv</a>
        / 
        <a href="https://github.com/fqnchina/DecoupleLearning">codes</a>
        /
        <a href="./QingnanFan_files/eccv_2018_supp.pdf">supp file</a>
        /
        <a href="./QingnanFan_files/eccv_2018_poster.pdf">poster</a>
        /
        <a href="./QingnanFan_files/eccv_2018.bib">bibtex</a>
        <p></p>
        <p> The first decouple learning framework that is capable of successfully incorporating many different parameterized image operators into a single network without requirement of retraining or fintuning any other networks. </p>
      </td>
    </tr>


    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
          <img src="./QingnanFan_files/cvpr18_thumbnail.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/cvpr18_thumbnail.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
      <a href="./QingnanFan_files/cvpr_2018.pdf">
            <papertitle>Revisiting Deep Intrinsic Image Decompositions</papertitle>
      </a>
      <br>
        <strong>Qingnan Fan</strong>, 
        <a href="http://jlyang.org/">Jiaolong Yang</a>, 
        <a href="https://www.microsoft.com/en-us/research/people/ganghua/">Gang Hua</a>, 
        <a href="http://www.cs.sdu.edu.cn/~baoquan/">Baoquan Chen</a>,
        <a href="http://www.davidwipf.com/">David Wipf</a>.

        <br>
        <em>CVPR</em>, 2018 <strong style="color:red">(Oral)</strong><br>
        <a href="https://arxiv.org/abs/1701.02965">arXiv</a>
        / 
        <a href="https://github.com/fqnchina/IntrinsicImage">codes</a>
        /
        <a href="./QingnanFan_files/cvpr_2018_v4_JL.pptx">slides</a>
        /  
        <a href="./QingnanFan_files/cvpr_2018_supp.pdf">supp file</a>
        /
        <a href="./QingnanFan_files/cvpr_2018_poster.pdf">poster</a>
        / 
        <a href="https://www.youtube.com/watch?v=LBJ20kxr1a0&t=3026s">presentation (start from 36:44)</a>
        /
        <a href="./QingnanFan_files/cvpr_2018.bib">bibtex</a>
        <p></p>
        <p>The first demonstration of a single basic deep architecture capable of achieving state-of-the-art results when applied to each of the major intrinsic benchmarks. </p>
      </td>
    </tr>


     <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
          <img src="./QingnanFan_files/iccv17_thumbnail.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/iccv17_thumbnail.png" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
      <a href="./QingnanFan_files/iccv_2017.pdf">
            <papertitle>A Generic Deep Architecture for Single Image Reflection Removal and Image Smoothing</papertitle>
      </a>
      <br>
        <strong>Qingnan Fan</strong>, 
        <a href="http://jlyang.org/">Jiaolong Yang</a>, 
        <a href="https://www.microsoft.com/en-us/research/people/ganghua/">Gang Hua</a>, 
        <a href="http://www.cs.sdu.edu.cn/~baoquan/">Baoquan Chen</a>,
        <a href="http://www.davidwipf.com/">David Wipf</a>.

        <br>
        <em>ICCV</em>, 2017 <br>
        <a href="https://arxiv.org/abs/1708.03474">arXiv</a>
        / 
        <a href="https://github.com/fqnchina/CEILNet">codes</a>
        / 
        <a href="./QingnanFan_files/iccv_2017_supp.pdf">supp file</a>
        / 
        <a href="./QingnanFan_files/iccv_2017_poster.pdf">poster</a>
        / 
        <a href="./QingnanFan_files/iccv_2017.bib">bibtex</a>
        <p></p>
        <p>An advanced deep architecture for low-level vision tasks; A novel reflection image synthesis approach which enables outstanding generalization ability to real images with trained newtork. </p>
      </td>
    </tr>


    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/siga_2015.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <img src="./QingnanFan_files/siga_2015.gif" width="175" alt="" style="border-style: none" align="middle">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
      <a href="https://www.dropbox.com/s/q2xvrag0g5l8wii/%5B2015%5D%5Bsiggraph_asia%5DJumpCut%20Non-Successive%20Mask%20Transfer%20and%20Interpolation%20for%20Video%20Cutout.pdf?dl=0">
            <papertitle>JumpCut: Non-Successive Mask Transfer and Interpolation for Video Cutout</papertitle>
      </a>
      <br>
            <strong>Qingnan Fan</strong>,
            <a href="http://vr.sdu.edu.cn/~zf/">Fan Zhong</a>,
            <a href="http://www.cs.huji.ac.il/~danix/">Dani Lischinski</a>,
            <a href="http://www.math.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
            <a href="http://www.cs.sdu.edu.cn/~baoquan/">Baoquan Chen</a>.
        <br>
        <em>SIGGRAPH Asia</em>, 2015 & <em>TOG</em>, 2015 <br>
        <a href="https://github.com/sduirc/JumpCut">codes</a>
        / 
        <a href="https://www.dropbox.com/scl/fi/jr84cmwryf50vnb61wb6e/siga_2015.pptx?dl=0&rlkey=9jn17dt75d0g2bi23wlax5hb6">slides</a>
        / 
        <a href="https://www.youtube.com/watch?v=drqnwDg0JFM&t=77s">video</a>
        / 
        <a href="https://www.dropbox.com/s/4b76crsifryn6rq/siga_2015_supp.rar?dl=0">supp file</a>
        / 
        <a href="https://www.dropbox.com/s/v0v3pkrhz1vizyt/VideoSeg_dataset.rar?dl=0">dataset</a>
        / 
        <a href="./QingnanFan_files/siga_2015.bib">bibtex</a>
        <!-- /
        <a href="http://irc.cs.sdu.edu.cn/JumpCut/">deprecated project page</a> -->
        <p></p>
        <p>An interactive real-time video segmentation algorithm. Significantly improve the video cutout accuracy and efficiency.</p>
      </td>
    </tr>


    <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
      <td width="25%">
        <div class="one">
        <div class="two" id="portrait_image" style="opacity: 0;">
        <img src="./QingnanFan_files/a97-lu.gif" width="110" alt="" style="border-style: none" align="right">
        </div>
        <img src="./QingnanFan_files/a97-lu.gif" width="110" alt="" style="border-style: none" align="right">
        </div>
        <script type="text/javascript">
        function portrait_start() {
        document.getElementById('portrait_image').style.opacity = "1";
        }
        function portrait_stop() {
        document.getElementById('portrait_image').style.opacity = "0";
        }
        portrait_stop()
        </script>
      </td>
      <td valign="top" width="75%">
      <a href="https://www.dropbox.com/s/f84dqmtht41k8vv/a97-lu.pdf?dl=0">
            <papertitle>Build-to-Last: Strength to Weight 3D Printed Objects</papertitle>
      </a>
      <br>
            <a href="http://vr.sdu.edu.cn/~lulin/">Lin Lu</a>,
            <a href="http://www.cs.bgu.ac.il/~asharf/">Andrei Sharf</a>,
            Haisen Zhao, Yuan Wei,
            <strong>Qingnan Fan</strong>, Xuelin Chen,
            <a href="http://www.animlife.com/">Yann Savoye</a>,
            <a href="http://www.cs.sdu.edu.cn/zh/60">Changhe Tu</a>,
            <a href="http://www.math.tau.ac.il/~dcor/">Daniel Cohen-Or</a>,
            <a href="http://www.cs.sdu.edu.cn/~baoquan/">Baoquan Chen</a>.
        <br>
        <em>SIGGRAPH</em>, 2014 & <em>TOG</em>, 2014 <br>
        <a href="https://www.youtube.com/watch?v=V5IrPSvcm_8&t=5s">video</a>
        / 
        <a href="./QingnanFan_files/sig_2014.bib">bibtex</a>
        <p></p>
        <p>Reduce the material cost and weight of a given object while providing a durable printed model that is resistant to impact and external forces.</p>
      </td>
    </tr>

  </tbody></table>
<!-- 
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td width="100%" valign="middle">
        <strong><heading>Interesting research projects</heading></strong>
      </td>
    </tr>
    </tbody></table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

      <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
        <td width="25%">
          <div class="one">
          <div class="two" id="portrait_image" style="opacity: 0;">
          <img src="./QingnanFan_files/arxiv23-reflection.png" width="175" alt="" style="border-style: none" align="middle">
          </div>
          <img src="./QingnanFan_files/arxiv23-reflection.png" width="175" alt="" style="border-style: none" align="middle">
          </div>
          <script type="text/javascript">
          function portrait_start() {
          document.getElementById('portrait_image').style.opacity = "1";
          }
          function portrait_stop() {
          document.getElementById('portrait_image').style.opacity = "0";
          }
          portrait_stop()
          </script>
        </td>
        <td valign="top" width="75%">
              <papertitle>Single image reflection removal via learning with multi-image constraints</papertitle>
      <br>
      <a href="https://yd-yin.github.io/">Yingda Yin*</a>, 
      <strong>Qingnan Fan*</strong>, 
      <a href="http://www.dongdongchen.bid/">Dongdong Chen</a>, 
      <a href="http://yujiewang.info/">Yujie Wang</a>, 
      <a href="https://angelicaiaviles.wordpress.com/">Angelica Aviles-Rivero</a>,
      <a href="https://liruoteng.github.io/">Ruoteng Li</a>, 
      <a href="http://www.damtp.cam.ac.uk/user/cbs31/Home.html">Carola-Bibiane Schönlieb</a>,
      <a href="http://www.cs.sdu.edu.cn/~baoquan/">Baoquan Chen</a>.
      <br>
      2023
      <br>
          <a href="https://arxiv.org/abs/1912.03623">Arxiv</a>
          <p> Our algorithm works by learning a deep neural network to optimize the target with joint constraints enhanced among multiple input images during the training phase, but is able to eliminate reflections only from a single input for evaluation.</p>
        </td>
      </tr>


      <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
        <td width="25%">
          <div class="one">
          <div class="two" id="portrait_image" style="opacity: 0;">
          <img src="./QingnanFan_files/arxiv-RoboAssembly.png" width="175" alt="" style="border-style: none" align="middle">
          </div>
          <img src="./QingnanFan_files/arxiv-RoboAssembly.png" width="175" alt="" style="border-style: none" align="middle">
          </div>
          <script type="text/javascript">
          function portrait_start() {
          document.getElementById('portrait_image').style.opacity = "1";
          }
          function portrait_stop() {
          document.getElementById('portrait_image').style.opacity = "0";
          }
          portrait_stop()
          </script>
        </td>
        <td valign="top" width="75%">
              <papertitle>RoboAssembly: Learning Generalizable Furniture Assembly Policy in a Novel Multi-robot Contact-rich Simulation Environment</papertitle>
      <br>
      <a href="https://mingxiny.github.io/">Mingxin Yu*</a>, 
      <a href="https://linsats.github.io/">Lin Shao*</a>, 
      <a href="https://acmlczh.github.io/">Zhehuan Chen</a>, 
      <a href="https://tianhaowuhz.github.io/" target="_blank">Tianhao Wu</a>,
      <strong>Qingnan Fan</strong>, 
      <a href="https://cs.stanford.edu/~kaichun/">Kaichun Mo</a>, 
      <a href="https://zsdonghao.github.io/" target='_blank'>Hao Dong</a>.
      <br>
      2021
      <br>
          <a href="https://arxiv.org/abs/2112.10143">Arxiv</a> /
          <a href="https://sites.google.com/view/roboticassembly">Project page</a>
          <p> We formulate the part assembly task as a concrete reinforcement learning problem and propose a pipeline for robots to learn to assemble a diverse set of chairs.</p>
        </td>
      </tr>


      <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
        <td width="25%">
          <div class="one">
          <div class="two" id="portrait_image" style="opacity: 0;">
          <img src="./QingnanFan_files/IF-Defense.png" width="175" alt="" style="border-style: none" align="middle">
          </div>
          <img src="./QingnanFan_files/IF-Defense.png" width="175" alt="" style="border-style: none" align="middle">
          </div>
          <script type="text/javascript">
          function portrait_start() {
          document.getElementById('portrait_image').style.opacity = "1";
          }
          function portrait_stop() {
          document.getElementById('portrait_image').style.opacity = "0";
          }
          portrait_stop()
          </script>
        </td>
        <td valign="top" width="75%">
              <papertitle>IF-Defense: 3D Adversarial Point Cloud Defense via Implicit Function based Restoration</papertitle>
      <br>
      <a href="https://wuziyi616.github.io/">Ziyi Wu</a>, 
      <a href="https://geometry.stanford.edu/person.php?id=duanyq19">Yueqi Duan</a>, 
      <a href="https://hughw19.github.io/">He Wang</a>, 
      <strong>Qingnan Fan</strong>, 
      <a href="http://geometry.stanford.edu/member/guibas/index.html" target="_blank">Leonidas Guibas</a>.
      <br>
      2020
      <br>
          <a href="https://arxiv.org/abs/2010.05272">Arxiv</a> /
          <a href="https://github.com/Wuziyi616/IF-Defense">Codes</a>
          <p> We propose an IF-Defense framework to directly optimize the coordinates of input points with geometry-aware and distribution-aware constraints, to address the 3D adversarial attacks.</p>
        </td>
      </tr>

      <tr onmouseout="portrait_stop()" onmouseover="portrait_start()">
        <td width="25%">
          <div class="one">
          <div class="two" id="portrait_image" style="opacity: 0;">
          <img src="./QingnanFan_files/P4Contrast.png" width="175" alt="" style="border-style: none" align="middle">
          </div>
          <img src="./QingnanFan_files/P4Contrast.png" width="175" alt="" style="border-style: none" align="middle">
          </div>
          <script type="text/javascript">
          function portrait_start() {
          document.getElementById('portrait_image').style.opacity = "1";
          }
          function portrait_stop() {
          document.getElementById('portrait_image').style.opacity = "0";
          }
          portrait_stop()
          </script>
        </td>
        <td valign="top" width="75%">
              <papertitle>P4Contrast: Contrastive Learning with Pairs of Point-Pixel Pairs for RGB-D Scene Understanding</papertitle>
      <br>
      Yunze Liu,
      <a href="https://ericyi.github.io/">Li Yi</a>,
      <a href="https://www.shanghangzhang.com/">Shanghang Zhang</a>, 
      <strong>Qingnan Fan</strong>, 
      <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>,
      <a href="https://zsdonghao.github.io/">Hao Dong</a>.
      <br>
      2020
      <br>
          <a href="https://arxiv.org/abs/2012.13089">Arxiv</a>
          <p> We propose contrasting “pairs of point-pixel pairs”, where positives include pairs of RGB-D points in correspondence, and negatives include pairs where one of the two modalities has been disturbed and/or the two RGB-D points are not in correspondence.</p>
        </td>
      </tr>

    </tbody></table> -->



  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td width="100%" valign="middle">
        <strong><heading>Work experience</heading></strong>
      </td>
    </tr>
    </tbody></table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="25%">
          <img src="./QingnanFan_files/vivo.jpg" width="110" alt="" style="border-style: none" align="right">
        </td>
        <td valign="top" width="75%">
          <papertitle>VIVO</papertitle>
          <br>
          Lead Researcher
          <br>
          2023-now
        </td>
      </tr>

      <tr>
        <td width="25%">
          <img src="./QingnanFan_files/tencent.png" width="110" alt="" style="border-style: none" align="right">
        </td>
        <td valign="top" width="75%">
          <papertitle>Tencent AI Lab</papertitle>
          <br>
          Senior Researcher
          <br>
          2021-2023
        </td>
      </tr>
  </table>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td width="100%" valign="middle">
        <strong><heading>Education</heading></strong>
      </td>
    </tr>
    </tbody></table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="25%">
          <img src="./QingnanFan_files/stanford.jpg" width="110" alt="" style="border-style: none" align="right">
        </td>
        <td valign="top" width="75%">
          <papertitle>Stanford University</papertitle>
          <br>
          Postdoctoral Researcher
          <br>
          Supervised by Prof. Leonidas Guibas
          <br>
          2019-2021
        </td>
      </tr>

      <tr>
        <td width="25%">
          <img src="./QingnanFan_files/SDU.jpg" width="110" alt="" style="border-style: none" align="right">
        </td>
        <td valign="top" width="75%">
          <papertitle>Shandong University</papertitle>
          <br>
          Ph.D. student
          <br>
          Supervised by Prof. Baoquan Chen
          <br>
          2014-2019
        </td>
      </tr>

      <tr>
        <td width="25%">
          <img src="./QingnanFan_files/SDU.jpg" width="110" alt="" style="border-style: none" align="right">
        </td>
        <td valign="top" width="75%">
          <papertitle>Shandong University</papertitle>
          <br>
          Undergraduate student
          <br>
          2010-2014
        </td>
      </tr>
  </table>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td width="100%" valign="middle">
        <strong><heading>Research experience</heading></strong>
      </td>
    </tr>
    </tbody></table>

  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

    <tr>
      <td width="25%">
        <img src="./QingnanFan_files/bfa.jpg" width="110" alt="" style="border-style: none" align="right">
      </td>
      <td valign="top" width="75%">
        <papertitle>Beijing Film Academy</papertitle>
        <br>
        Research Intern
        <br>
        Supervised by Prof. Baoquan Chen
        <br>
        2018-2019
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src="./QingnanFan_files/cambridge.jpg" width="110" alt="" style="border-style: none" align="right">
      </td>
      <td valign="top" width="75%">
        <papertitle>University of Cambridge</papertitle>
        <br>
        Visiting Student
        <br>
        Supervised by Prof. Carola-Bibiane Schönlieb
        <br>
        2018
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src="./QingnanFan_files/msra.png" width="110" alt="" style="border-style: none" align="right">
      </td>
      <td valign="top" width="75%">
        <papertitle>Microsoft Research Asia</papertitle>
        <br>
        Research Intern
        <br>
        Supervised by Gang Hua, Xin Tong, Jiaolong Yang and David Wipf
        <br>
        2016-2018
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src="./QingnanFan_files/tau.jpg" width="110" alt="" style="border-style: none" align="right">
      </td>
      <td valign="top" width="75%">
        <papertitle>Tel Aviv University</papertitle>
        <br>
        Visiting Student
        <br>
        Supervised by Prof. Daniel Cohen-Or
        <br>
        2015
      </td>
    </tr>

    <tr>
      <td width="25%">
        <img src="./QingnanFan_files/The Hebrew University Jerusalem.png" width="110" alt="" style="border-style: none" align="right">
      </td>
      <td valign="top" width="75%">
        <papertitle>The Hebrew University of Jerusalem</papertitle>
        <br>
        Visiting Student
        <br>
        Supervised by Prof. Dani Lischinski
        <br>
        2014
      </td>
    </tr>
  </table>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td width="100%" valign="middle">
        <strong><heading>Talks</heading></strong>
        <p>APR. 2022: Active 3D scene understanding and its applications
          <br>“三维视觉与智能图形”前沿论坛, 图图名师讲堂, China
        </p>

        <p>OCT. 2021: Visual Localization
          <br>Embodied AI Workshop, Valse, China
        </p>

        <p>JAN. 2019: Deep Learning in Computational Photography
          <br>USC ICT/UW Reality Lab/Berkeley/Stanford/Google/MSR, US
        </p>  

        <p>DEC. 2018: Deep Learning for Single Image Artifact Removal
          <br>ACCV Tutorial 2018, Australia
        </p>

        <p>DEC. 2018: Image Smoothing via Unsupervised Learning
          <br>SIGGRAPH Asia 2018, Japan; GAMES Webinar, China
        </p>

        <p>AUG. 2018: Discovering Unsupervised Learning in Image Processing
          <br>CIA, Cambridge University, UK
        </p>

        <p>JUN. 2018: Revisiting Deep Intrinsic Image Decomposition
          <br>CVPR 2018, USA
        </p>

        <p>NOV. 2015: Interactive Real-time Video Segmentation
          <br>SIGGRAPH Asia 2015, Japan
        </p>
      </td>
    </tr>
    </tbody></table>


  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td width="100%" valign="middle">
        <strong><heading>Awards</heading></strong>
        <p>2022: Tencent Outstanding Contributor</p>
        <p>2020: CCF Doctorial Dissertation Award Nominee (CCF 优博提名)</p>
        <p>2019: Outstanding Academic Achievement Award of Shandong University</p>
        <p>2018: Academic Star Nominee of Shandong University (10/20000)</p>
        <p>2018: National Scholarship</p>
        <p>2016: Outstanding Academic Achievement Award of Shandong University</p>
        <p>2015: Presidential Scholarship of Shandong University (35/20000)
        (Highest honor for students in SDU, only 35 elected among around 20000 candidates)</p>
        <p>2015: National Scholarship</p>
        <p>2015: Pacemaker to Outstanding Graduate Student of Shandong University</p>
      </td>
    </tr>
    </tbody></table>


  <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tbody><tr>
      <td width="100%" valign="middle">
        <heading>Mentored students</heading>
          <p>Qihang Fang (PhD student in University of Hong Kong)</p>
          <p>Siyan Dong (PostDoc in University of Hong Kong)</p>
          <p>Yingda Yin (PhD student in Peking University)</p>
          <p>Kai Ye (PhD student in Peking University)</p>
          <p>Jiazhao Zhang (PhD student in Peking University)</p>
          <p>Yunze Liu (PhD student in Tsinghua University)</p>
          <p>Yijia Weng (PhD student in Stanford University)</p>
          <p>Jialei Huang (PhD student in Tsinghua University)</p>
          <p>Guanqi Zhan (PhD student in Oxford University)</p>
      </td>
    </tr>
    </tbody></table> -->
  

</body></html>